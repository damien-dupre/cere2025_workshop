---
title: "Identifying Correct or Incorrect Emotion Recognition from Facial Expression Time Series using R"
author: "Damien Dupr√©"
format: 
  revealjs:
    theme: [theme/metropolis.scss]
    slide-number: true
    logo: "img/logo.jpg"

execute: 
  echo: true
  warning: false
  
title-slide-attributes:
    data-background-iframe: grid-worms/index.html
preload-iframes: true
---

```{r}
#| label: setup
#| include: false

# libraries --------------------------------------------------------------------
library(knitr)
library(countdown)
library(fontawesome)
```

## Introduction

Facial Expression Recognition has advanced significantly with the **emergence of automatic emotion recognition systems**.

However, the outputs of these systems can be difficult to handle:

- The sheer volume of data from **high-frequency time recordings**,
- The presence of **autocorrelation** over time,
- The inclusion of **multiple variables**.

In this workshop, we will explore how to process and analyse such data using R.

## Case study

- **78 participants** (20 male, 58 female) have been recorded while they watched **six videos/tasks** designed to elicit specific emotions.

- A total of 467 video recordings were obtained (one participant completed only five videos) and analysed using **Affectiva‚Äôs Affedex** Emotion Recognition system integrated within the iMotions Lab software.

- Each video frame was scored for the **likelihood of expressing** the following emotions: Happiness, Surprise, Disgust, Fear, Sadness, and Anger, with values ranging from 0 (not recognised) to 1 (fully recognised).

:::{.center}
**Did the participants express the emotion intended by the elicitation tasks?**
:::

## Objectives

![](https://tidyverse.tidyverse.org/articles/data-science.png)

1. Import, Tidy and Transform
2. Visualise
3. Model
4. Communicate

# Technological Choices

## R not Python

There is little difference between R and Python for research purposes, but **R is, in my view, easier to read and write**.

This workshop assumes some familiarity with R, particularly:

- The **‚Äútidyverse‚Äù** coding style
- Use of the **native pipe** operator `|>` rather than the `%>%` pipe from the {magrittr} package

::: {.callout-note}
The pipe operator applies the object on the left-hand side to the first argument of the function on the right-hand side.

So instead of writing `f(arg1 = x, arg2 = y)`, you write `x |> f(arg2 = y)`.
:::

## Using RStudio in Posit Cloud

Although you may use your own R installation, there are excellent and **free cloud-based options**:

- Google Colab with Jupyter Notebook
- GitHub Codespaces with Visual Studio Code
- Posit Cloud with RStudio

::: {.callout-warning}
The free tier on Posit Cloud provides only 25 hours of usage per month.
:::

## üõ†Ô∏è Now, it‚Äôs Your Turn!

1.	In your browser, sign up or log in at <https://posit.cloud>
2.	Click on **New Project** and choose **New Project from Git Repository**
3.	Enter https://github.com/damien-dupre/cere2025_workshop when prompted for the repository URL

```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```

## Ressources

- All 467 csv files are in the data folder
- All R scripts used are in the scripts folder
- Slides and supplementary material are in the output folder

[]{.underline}

    cere2025_workshop/
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ F001_Angry.csv
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ F001_Disgust.csv
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ...
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ data_processing.R
    ‚îî‚îÄ‚îÄ output/
        ‚îú‚îÄ‚îÄ slides.html
        ‚îú‚îÄ‚îÄ slides.qmd
        ‚îî‚îÄ‚îÄ slides_files/
        
# 1. Import, Tidy and Transform

## Import

Start by installing and loading the necessary packages:

```{r}
# Install Packages (to run only once) ------------------------------------------
# install.packages("tidyverse")
# install.packages("fs")
# install.packages("here")
# install.packages("report")

# Load Packages ----------------------------------------------------------------
library(tidyverse)
library(fs)
library(here)
library(report)
```

We will combine all csv files into a single table:

```{r}
df <- 
  dir_ls(path = here("data"), glob = "*.csv") |> # list all csv files in folder "data"
  map_dfr(read_csv, .id = "source") # read the files and merge them one after the other

# write_csv(df, "data_all.csv")
```

## Import

Preview the df object:

```{r}
head(df, 10)
```

## Tidy and Transform

We will update the source variable to retain only the file name:

```{r}
df_tidy <- df |> 
  mutate(file = source |> path_file() |> path_ext_remove(), .keep	= "unused") |> 
  separate(col = file, into = c("ppt", "elicitation"), sep = "_", remove = FALSE) |> 
  mutate(elicitation = elicitation |> tolower() |> str_replace("happy", "happiness"))
```

Importantly, we need to transform this wide dataframe (i.e., all emotion variables are side by side) to a long dataframe (i.e., all emotion variables are below each others and only one "value" variable is used):

```{r}
df_tidy_long <- df_tidy |> 
  pivot_longer(
    cols = c(anger, disgust, fear, happiness, sadness, surprise),
    names_to = "emotion",
    values_to = "recognition"
  )
```

## Tidy and Transform

Let's have a look at the `df` object again:

```{r}
head(df, 10)
```

## üõ†Ô∏è Now, it‚Äôs Your Turn!

1. Open ‚Äúimport_tidy_transform.R‚Äù in the scripts folder
2. Select all the lines and click on the Run icon or CTRL + ENTER (Windows) / Command + ENTER (Mac)
3. Observe your `df_tidy_long` object from the Environment pane

```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```

# 2. Visualise

## Single Recording

Let‚Äôs visualise a single video:

```{r}
list_file <- unique(df_tidy_long$file)

df_tidy_long |> 
  filter(file == list_file[3]) |> 
  ggplot() +
  aes(x = frame, y = recognition, colour = emotion) +
  geom_line(linewidth = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_brewer(palette = "Dark2")
```

## All Recordings

Let‚Äôs visualise recordings for each elicitation tasks:

```{r}
list_elicitation <- unique(df_tidy_long$elicitation)

df_tidy_long |> 
  filter(elicitation == list_elicitation[3]) |> 
  ggplot() +
  aes(x = frame, y = recognition, group = ppt, colour = emotion) +
  geom_line(linewidth = 1) +
  facet_grid(emotion ~ ., switch = "x") +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1)) +
  scale_color_brewer(palette = "Dark2")
```

## üõ†Ô∏è Now, it‚Äôs Your Turn!

1. Open ‚Äúvisualise.R‚Äù in the scripts folder
2. Change the index numbers in the first two lines to explore different data
3. Select all the lines and click on the Run icon or CTRL + ENTER (Windows) / Command + ENTER (Mac)

```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```

# 3. Model

## Model

For each video, we aim to identify the expressed emotion using three distinct methods (as described in Dupr√©, 2021):

1.	Matching Score: Emotion with the highest single value
2.	Confidence Score: Emotion with the highest average value
3.	Frame Score: Emotion most frequently recognised across frames

## Model

Here is a visual representation of each method applied to a special case in which all methods return the same emotion recognised.

![](img/method_comp1.png)

## Model

However, some cases are returning different results:

![](img/method_comp2.png)

![](img/method_comp3.png)

## Disclaimer

These methods all have their pros and cons. For example, Confidence Score and Frame Score might be more robust to artefacts.

Additional calculation methods not presented here might be possible as well.

Finally, the categorisation of a whole video to a single label is a reductive approach that can be debated but it is not the topic of the workshop.

## Matching Score

The emotion recognised is the one having the highest value in the recording

```{r}
df_score_matching <- df_tidy_long |> 
  select(file, frame, emotion, recognition) |> 
  group_by(file) |> 
  filter(recognition == max(recognition)) |> 
  add_count() |> 
  mutate(emotion = case_when(n != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, emotion) |> 
  distinct() |> 
  mutate(score = "matching")
```

## Confidence Score

The emotion recognised is the one with the highest average along all the recording among the possible emotions

```{r}
df_score_confidence <- df_tidy_long |> 
  select(file, frame, emotion, recognition) |> 
  group_by(file, emotion) |> 
  summarise(mean_emotion = mean(recognition, na.rm = TRUE)) |> 
  slice_max(mean_emotion) |> 
  add_count() |> 
  mutate(emotion = case_when(n != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, emotion) |> 
  distinct() |> 
  mutate(score = "confidence")
```

## Frame Score

Identify the emotion recognised in each frame (max value) and to count how many time each have been recognised in a video

```{r}
df_score_frame <- df_tidy_long |> 
  select(file, frame, emotion, recognition) |> 
  group_by(file, frame) |> 
  slice_max(recognition) |> 
  add_count(name = "n_frame") |> 
  mutate(emotion = case_when(n_frame != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, frame, emotion) |> 
  distinct() |> 
  group_by(file, emotion) |> 
  count() |> 
  group_by(file) |> 
  slice_max(n) |> 
  add_count(name = "n_file") |> 
  mutate(emotion = case_when(n_file != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, emotion) |> 
  distinct() |> 
  mutate(score = "frame")
```

## Comparing Scores

Now a label has been assigned to each recorded video using 4 different calculation methods, we can compare these score with the "ground truth" (i.e., the type of emotion supposedly elicited). 

```{r}
df_congruency <- 
  bind_rows(
    df_score_matching, 
    df_score_confidence,
    df_score_frame
  ) |> 
  separate(col = file, into = c("ppt", "elicitation"), sep = "_", remove = FALSE) |> 
  mutate(
    elicitation = elicitation |> tolower() |> 
      str_replace_all(c("happy" = "happiness", "sad" = "sadness", "angry" = "anger")),
    congruency = if_else(elicitation == emotion, 1, 0)
  )
```

## Comparing Scores

```{r}
model <- df_congruency |> 
  glm(
    congruency ~ score*elicitation, 
    data = _, 
    family = binomial
  ) |> 
  aov()
```

# 4. Communicate

## Effects Visualisation

```{r}
df_congruency |> 
  group_by(score, elicitation) |> 
  summarise(mean_se(congruency)) |> 
  ggplot() + 
  aes(
    x = elicitation,
    y = y,
    ymin = ymin,
    ymax = ymax,
    fill = score,
    shape = score
  ) +
  geom_errorbar(position = position_dodge(width = 0.8)) +
  geom_point(size = 4, position = position_dodge(width = 0.8))
```

## Effects Visualisation

```{r}
df_congruency |> 
  group_by(score, elicitation) |> 
  summarise(mean_se(congruency)) |> 
  ggplot() + 
  aes(
    x = fct_reorder(elicitation, y, .fun = "mean"),
    y = y,
    ymin = ymin,
    ymax = ymax,
    fill = score,
    shape = score
  ) +
  ggstats::geom_stripped_cols() +
  geom_errorbar(width = 0, position = position_dodge(width = 0.8)) +
  geom_point(stroke = 0, size = 4, position = position_dodge(width = 0.8)) +
  scale_y_continuous("Congruence between elicitation and recognition", limits = c(0, 1), labels = scales::percent) +
  scale_x_discrete("") +
  scale_fill_brewer("Method", palette = "Dark2") +
  scale_shape_manual("Method", values = c(21, 22, 23, 24)) +
  theme_bw() +
  theme(
    text = element_text(size = 15),
    axis.text.x = element_text(size = 16),
    axis.text.y = element_text(size = 16),
    axis.line.y = element_blank(),
    legend.title.align = 0.5,
    legend.position = c(0.85, 0.2),
    legend.background = element_rect(fill = "grey80")
  ) +
  coord_flip(ylim = c(0, 1)) +
  guides(
    shape = guide_legend(reverse = TRUE),
    fill = guide_legend(reverse = TRUE)
  )
```

## Effects Statistics

```{r}
report(model)
```

# 5. Discussion and Conclusion

## On Technology

- Tidyverse and the native |> pipe operator make code more readable and teachable
- Benefits include increased scientific transparency and reproducibility
- Being open-source encourages improved practices

## On Theory

- We cannot confirm whether the participants felt the emotion elicited, only that it may or may not have been expressed (Tcherkassof and Dupr√©, 2020)
- A lack of emotional expression does not necessarily indicate an issue with the participant, it could be due to recognition system limitations

## On Methods

- All three methods compare emotions relative to others. A low score might still be the highest and thus chosen
- A minimum threshold should be introduced for more valid recognition

- Method performance differs:

  - Matching Score struggles with prolonged emotions like happiness and disgust
	- Frame Score underperforms for surprise, sadness, anger, and fear
	- Confidence Score, based on average values, appears the most robust overall
	
## {background="#43464B"}

```{css, echo = FALSE}
img.circle {border-radius:50%;}
```

::: {layout-ncol="2"}
<img class="circle" src="https://github.com/damien-dupre.png" />

**Thanks for your attention and don't hesitate to ask if you have any questions!**  
[`r fa(name = "mastodon")` @damien_dupre](https://datasci.social/@damien_dupre)  
[`r fa(name = "github")` @damien-dupre](https://github.com/damien-dupre)  
[`r fa(name = "link")` https://damien-dupre.github.io](https://damien-dupre.github.io)  
[`r fa(name = "paper-plane")` damien.dupre@dcu.ie](mailto:damien.dupre@dcu.ie)
:::