---
title: "Identifying Correct or Incorrect Emotion Recognition from Facial Expression Time Series using R"
author: "Damien DuprÃ©"
format: 
  revealjs:
    theme: [theme/metropolis.scss]
    slide-number: true
    logo: img/logo.png
    footer: Damien DuprÃ© - CERE2025

execute: 
  echo: true
  warning: false
  
title-slide-attributes:
    data-background-iframe: grid-worms/index.html
preload-iframes: true
---

```{r}
#| label: setup
#| include: false

# libraries --------------------------------------------------------------------
library(knitr)
library(countdown)
library(fontawesome)
```

## Introduction

Facial Expression Recognition has advanced significantly with the **emergence of automatic emotion recognition systems**.

However, the outputs of these systems can be difficult to handle:

- The sheer volume of data from **high-frequency time recordings**,
- The presence of **autocorrelation** over time,
- The inclusion of **multiple variables**.

In this workshop, we will explore how to process and analyse such data using R.

## Case study

- **78 participants** (20 males, 58 females) have been recorded while they were asked to express **six emotions** and constitute de [BU-4DFE database (Yin et al., 2008)](https://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html). 

- A total of 467 video recordings were obtained (one participant completed only five videos) and analysed using **Affectivaâ€™s Affedex** Emotion Recognition system integrated within the iMotions Lab software.

- Each video frame was scored for the **likelihood of expressing** the following emotions: Happiness, Surprise, Disgust, Fear, Sadness, and Anger, with values from 0 (not recognised) to 1 (fully recognised).

:::{.center}
**Did the participants express the emotion intended by the instructions?**
:::

## Objectives

![](https://tidyverse.tidyverse.org/articles/data-science.png)

1. Import, Tidy and Transform
2. Visualise
3. Model
4. Communicate

# Technological Choices

## R not Python

There is little difference between R and Python for research purposes, but **R is, in my view, easier to read and write**.

This workshop assumes some familiarity with R, particularly:

- The **â€œtidyverseâ€** coding style
- Use of the **native pipe** operator `|>` rather than the `%>%` pipe from the {magrittr} package

::: {.callout-note}
The pipe operator applies the object on the left-hand side to the first argument of the function on the right-hand side.

So instead of writing `f(arg1 = x, arg2 = y)`, you write `x |> f(arg2 = y)`.
:::

## Using RStudio in Posit Cloud

Although you may use your own R installation, there are excellent and **free cloud-based options**:

- Google Colab with Jupyter Notebook
- GitHub Codespaces with Visual Studio Code
- Posit Cloud with RStudio

::: {.callout-warning}
The free tier on Posit Cloud provides only 25 hours of usage per month.
:::

## ğŸ› ï¸ Now, itâ€™s Your Turn!

1.	In your browser, sign up or log in at <https://posit.cloud>
2.	Click on **New Project** and choose **New Project from Git Repository**
3.	Enter `https://github.com/damien-dupre/cere2025_workshop` when prompted for the repository URL

![](img/posit_cloud.png)

```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```

## Ressources

:::: {layout="[1,1]"}
::: {#first-column}
- All 467 csv files are in the `data/` folder
- All R scripts used are in the `scripts/` folder
- Slides and supplementary material are in the `output/` folder
:::

::: {#second-column}
[`cere2025_workshop/` folder structure]{.underline}

    cere2025_workshop/
    â”œâ”€â”€ data/
    â”‚Â Â  â”œâ”€â”€ F001_Angry.csv
    â”‚Â Â  â”œâ”€â”€ F001_Disgust.csv
    â”‚Â Â  â””â”€â”€ ...
    â”œâ”€â”€ scripts/
    â”‚Â Â  â”œâ”€â”€ import_tidy_transform.R
    â”‚Â Â  â”œâ”€â”€ visualise.R
    â”‚Â Â  â”œâ”€â”€ model.R
    â”‚Â Â  â””â”€â”€ communicate.R
    â””â”€â”€ output/
        â”œâ”€â”€ slides.html
        â”œâ”€â”€ slides.qmd
        â”œâ”€â”€ slides_files/
        â””â”€â”€ ...
        
:::
::::

# 1. Import, Tidy and Transform

## Import

Start by installing the necessary packages:

```{r}
#| eval: false

install.packages("tidyverse") # Metapackage for data transformation and visualisation
install.packages("fs") # Manipulate files' and folders' path
install.packages("here") # Rebase the origin of the repository regardless of the system
install.packages("report") # Standardize the output of statistical models
```

And load them: 

```{r}
library(tidyverse)
library(fs) 
library(here) 
library(report)
```

## Import

We will combine all .csv files into a single data frame:

```{r}
df <- 
  dir_ls(path = here("data"), glob = "*.csv") |> # list all csv files in folder "data"
  map_dfr(read_csv, .id = "source") # read the files and merge them one after the other

# write_csv(df, "data_all.csv")
```

Preview the 5 first rows of the `df` object:

```{r}
head(df, 5)
```

## Tidy and Transform

We will update the source variable to retain only the file name:

```{r}
df_tidy <- df |> 
  mutate(file = source |> path_file() |> path_ext_remove(), .keep	= "unused") |> 
  separate(col = file, into = c("ppt", "instruction"), sep = "_", remove = FALSE)
```

Importantly, we need to transform this wide dataframe (i.e., all emotion variables are side by side) to a long dataframe (i.e., all emotion variables are below each others and only one "value" variable is used):

```{r}
df_tidy_long <- df_tidy |> 
  pivot_longer(
    cols = c(anger, disgust, fear, happiness, sadness, surprise),
    names_to = "emotion",
    values_to = "recognition"
  )
```

## Tidy and Transform

The `df_tidy_long` object has `r nrow(df_tidy_long)` rows, let's look at its first rows:

```{r}
head(df_tidy_long, 10)
```

## ğŸ› ï¸ Now, itâ€™s Your Turn!

1. Open â€œ1_import_tidy_transform.Râ€ in the `scripts/` folder
2. Select all the lines and click on the **Run** icon (top left)
3. Observe your `df_tidy_long` object

::: {.callout-note}
Instead of clicking on the Run you can also press:

- <kbd>CTRL</kbd> + <kbd>ENTER</kbd> (Windows)
- <kbd>Command</kbd> + <kbd>ENTER</kbd> (Mac)
:::


```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```

# 2. Visualise

## Single Recording

Letâ€™s visualise a single video:

```{r}
#| output-location: column
#| results: hold
#| fig-width: 5
#| fig-height: 6

list_file <- unique(df_tidy_long$file)

df_tidy_long |> 
  filter(file == list_file[3]) |> 
  ggplot() +
  aes(
    x = frame, 
    y = recognition, 
    colour = emotion
  ) +
  geom_line(linewidth = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_brewer(palette = "Dark2")
```

## All Recordings for 1 Task

Letâ€™s visualise recordings for one instruction task:

```{r}
#| output-location: column
#| results: hold
#| fig-width: 5
#| fig-height: 6

list_task <- unique(df_tidy_long$instruction)

df_tidy_long |> 
  filter(instruction == list_task[3]) |> 
  ggplot() +
  aes(
    x = frame, 
    y = recognition, 
    group = ppt, 
    colour = emotion
  ) +
  geom_line(linewidth = 1, alpha = 0.2) +
  facet_grid(emotion ~ ., switch = "x") +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  scale_color_brewer(palette = "Dark2") +
  guides(colour = "none")
```

## ğŸ› ï¸ Now, itâ€™s Your Turn!

1. Open â€œ2_visualise.Râ€ in the `scripts/` folder
2. **Change the index numbers** in the first two lines to explore different data

```{r}
#| eval: false

recording_n <- 3 # replace 3 by a number between 1 and 467
instruction_n <- 3 # replace 3 by a number between 1 and 6
```

3. Select all lines and click **Run** (or use keyboard shortcuts)

::: {.callout-warning}
The code in the script â€œ1_import_tidy_transform.Râ€ must have been ran before running the code in the script â€œ2_visualise.Râ€.
:::

```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```

# 3. Model

## Model

For each video, we aim to identify the expressed emotion using **three distinct methods** (as described in DuprÃ©, 2021):

1. **Matching Score**: Emotion with the highest single value
2. **Confidence Score**: Emotion with the highest average value
3. **Frame Score**: Emotion most frequently recognised across frames

## Model

Here is a visual representation of each method applied to a special case in which all methods return the same emotion recognised.

![](img/method_comp1.png)

## Model

However, some cases are returning different results:

![](img/method_comp2.png)

![](img/method_comp3.png)

## Disclaimer

Each of these methods has its **advantages and disadvantages**. For instance, Confidence Score and Frame Score may offer greater robustness against artefacts.

There may also be **alternative calculation methods** not included in this discussion.

Lastly, assigning a single label to an entire video is a **simplification that could be questioned**, though this issue lies outside the scope of the workshop.

## Matching Score

The emotion recognised is the one having the highest value in the recording

```{r}
df_score_matching <- df_tidy_long |> 
# keep only the frame with the highest value in each ties
  group_by(file) |> 
  filter(recognition == max(recognition)) |> 
# in case of ties, label the emotions "undetermined" and remove duplicates
  add_count() |> 
  mutate(emotion = case_when(n != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, emotion) |> 
  distinct() |> 
# label the method
  mutate(method = "matching score")
```

## Confidence Score

The emotion recognised is the one with the highest average along all the recording among the possible emotions

```{r}
df_score_confidence <- df_tidy_long |> 
# calculate the average value for each emotion in each file and keep the highest
  group_by(file, emotion) |> 
  summarise(mean_emotion = mean(recognition, na.rm = TRUE)) |> 
  slice_max(mean_emotion) |> 
# in case of ties, label the emotions "undetermined" and remove duplicates
  add_count() |> 
  mutate(emotion = case_when(n != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, emotion) |> 
  distinct() |> 
# label the method
  mutate(method = "confidence score")
```

## Frame Score

Identify the emotion recognised in each frame (max value) and to count how many time each have been recognised in a video

```{r}
df_score_frame <- df_tidy_long |> 
  group_by(file, frame) |> # in each file, for each frame, find the highest value
  slice_max(recognition) |> 
  add_count(name = "n_frame") |> # in case of ties, label the emotions "undetermined" and remove duplicates
  mutate(emotion = case_when(n_frame != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, frame, emotion) |> 
  distinct() |> 
  group_by(file, emotion) |> # count the occurrence of each emotion across all frames and select highest
  count() |> 
  group_by(file) |> 
  slice_max(n) |> 
  add_count(name = "n_file") |> # in case of ties, label the emotions "undetermined" and remove duplicates
  mutate(emotion = case_when(n_file != 1 ~ "undetermined", .default = emotion)) |> 
  select(file, emotion) |> 
  distinct() |> 
  mutate(method = "frame score") # label the method
```

## Comparing Scores

Now a label has been assigned to each recorded video using 3 different calculation methods, we can **compare these score with the "ground truth"** (i.e., the type of emotion supposedly elicited). 

```{r}
df_congruency <- 
  bind_rows(
    df_score_matching, 
    df_score_confidence,
    df_score_frame
  ) |> 
  separate(col = file, into = c("ppt", "instruction"), sep = "_", remove = FALSE) |> 
  mutate(
    instruction = instruction |> 
           tolower() |> 
           str_replace_all(c("happy" = "happiness", "sad" = "sadness", "angry" = "anger")),
    congruency = if_else(instruction == emotion, 1, 0)
  )
```

## Comparing Scores

A **Generalised Linear Model** using a binomial distribution of the residuals (logistic regression) is fitted to identify the effects of calculation methods, instruction tasks, and the interaction between both.

To obtain their omnibus effect estimates, an **analysis of variance** is used on the GLM model.

```{r}
model <- df_congruency |> 
  glm(
    congruency ~ method*instruction, 
    data = _, 
    family = binomial
  ) |> 
  aov()
```

## ğŸ› ï¸ Now, itâ€™s Your Turn!

1. Open â€œ3_model.Râ€ in the `scripts/` folder
2. Select all lines and click **Run** (or use keyboard shortcuts)

::: {.callout-warning}
The code in the script â€œ1_import_tidy_transform.Râ€ must have been ran before running the code in the script â€œ3_model.Râ€.
:::

```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```

# 4. Communicate

## Effects Visualisation

Let's calculate the average congruency by instruction task and by method with a basic visualisation:

```{r}
#| output-location: column
#| results: hold
#| fig-width: 5
#| fig-height: 6

df_congruency |> 
  group_by(method, instruction) |> 
  summarise(mean_se(congruency)) |> 
  ggplot() + 
  aes(
    x = instruction,
    y = y,
    ymin = ymin,
    ymax = ymax,
    fill = method,
    shape = method
  ) +
  geom_errorbar(position = position_dodge(width = 0.8)) +
  geom_point(
    size = 4, 
    position = position_dodge(width = 0.8)
  ) +
  theme_bw() +
  theme(legend.position = "bottom")
```

## Effects Visualisation

Here is the same visualisation but with more customisations:

```{r}
#| output-location: column
#| results: hold
#| fig-width: 5
#| fig-height: 6

df_congruency |> 
  group_by(method, instruction) |> 
  summarise(mean_se(congruency)) |> 
  ggplot() + 
  aes(
    x = fct_reorder(instruction, y, .fun = "mean"),
    y = y,
    ymin = ymin,
    ymax = ymax,
    fill = method,
    shape = method
  ) +
  ggstats::geom_stripped_cols() +
  geom_errorbar(width = 0, position = position_dodge(width = 0.8)) +
  geom_point(stroke = 0, size = 4, position = position_dodge(width = 0.8)) +
  scale_y_continuous("Congruence between instruction and recognition", limits = c(0, 1), labels = scales::percent) +
  scale_x_discrete("") +
  scale_fill_brewer("Method", palette = "Dark2") +
  scale_shape_manual("Method", values = c(21, 22, 23, 24)) +
  guides(
    shape = guide_legend(reverse = TRUE, position = "inside"),
    fill = guide_legend(reverse = TRUE, position = "inside")
  ) +
  theme_bw() +
  theme(
    text = element_text(size = 12),
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.line.y = element_blank(),
    legend.title = element_text(hjust = 0.5),
    legend.position.inside = c(0.8, 0.2),
    legend.background = element_rect(fill = "grey80")
  ) +
  coord_flip(ylim = c(0, 1)) 
```

## Effects Statistics

```{r}
#| output: asis

cat(report(model))
```

## ğŸ› ï¸ Now, itâ€™s Your Turn!

1. Open â€œ4_communicate.Râ€ in the `scripts/` folder
2. Select all lines and click **Run** (or use keyboard shortcuts)

::: {.callout-warning}
The code in the scripts â€œ1_import_tidy_transform.Râ€ and â€œ3_model.Râ€ must have been ran before running the code in the script â€œ4_communicate.Râ€.
:::

```{r}
#| echo: false

countdown(minutes = 2, warn_when = 60)
```


# 5. Discussion and Conclusion

## On Technology

- {Tidyverse} and the native `|>` pipe operator make **code more readable** and teachable
- Increased **scientific transparency and reproducibility**
- Being open-source **encourages improved practices and sharing methods**
- **Easier to spot mistakes** in the data processing

## On Theory

- Each participant was instructed by a psychologist to gradually portray the six basic emotions in distinct sequences, **their expression are not genuine** 
- Low congruence scores does not necessarily indicate an issue with the participants, it could be due to **recognition system limitations**

:::: {layout="[2,1]"}
::: {#first-column}
![](https://github.com/damien-dupre/pint_of_science/blob/main/img/interstellar_affdex.gif?raw=true)
:::

::: {#second-column}
[[Extract from a video posted by Affectiva on YouTube in 2016 to demonstrate the capabilities of their Affdex system.](https://www.youtube.com/watch?v=NsmAldoVwDs)]{style="font-size: 70%"}
:::
::::

## On Methods

- All three methods use a **relative indicator rather than an absolute threshold** to identify the emotion recognised:

  - A low score might still be the highest and thus chosen
  - A minimum threshold should be introduced for more valid recognition
  
- **Method performance differs**:

  - Matching Score struggles with prolonged expressions like happiness and disgust
  - Frame Score underperforms for surprise, sadness, anger, and fear
	- Confidence Score, based on average values, appears the most robust method overall
	
## {background="#43464B"}

```{css, echo = FALSE}
img.circle {border-radius:50%;}
```

::: {layout-ncol="2"}
<img class="circle" src="https://github.com/damien-dupre.png" />

**Thanks for your attention and don't hesitate to ask if you have any questions!**  
[`r fa(name = "mastodon")` @damien_dupre](https://datasci.social/@damien_dupre)  
[`r fa(name = "github")` @damien-dupre](https://github.com/damien-dupre)  
[`r fa(name = "link")` https://damien-dupre.github.io](https://damien-dupre.github.io)  
[`r fa(name = "paper-plane")` damien.dupre@dcu.ie](mailto:damien.dupre@dcu.ie)
:::